{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNH7hEeroFL+7UXJkJAn4w3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/saifEEE01/NLP_course/blob/main/Exam1_text_preprocess.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VuKf3BrjiSF_"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Scenario: AI Chatbot for Customer Service\n",
        "Context:\n",
        "You are developing an AI-powered chatbot for a large e-commerce website. Customers often ask questions like:\n",
        "\n",
        "‚ÄúHey, can u plz tell me where‚Äôs my order??‚Äù\n",
        "\n",
        "‚Äúi didn‚Äôt receive my parcel yet!!!!‚Äù\n",
        "\n",
        "‚ÄúWhr‚Äôs my ordr üò°üò°‚Äù\n",
        "\n",
        "‚Äúdelivery late af... i want refund now‚Äù\n",
        "\n",
        "Question:\n",
        "Design a preprocessing pipeline that will normalize these kinds of user inputs before feeding them into an intent classification model. What specific steps would you take to clean and standardize the text? Your solution should handle abbreviations, slang, emojis, misspellings, and emotions or other tex preprocessing techniques.\n",
        "üî∏ Bonus: How would you preserve emotional intensity (like frustration) in preprocessing without losing model accuracy?\n",
        "\n",
        "Submission: share github link and read me where you each line of your code properly. Dont use chatgpt for coding. Use google or other things.\n"
      ],
      "metadata": {
        "id": "cvmt7ERkUNCs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Let's assume we have a csv file named \"customer_queries.csv\" where each line contains a separate customer's query.\n",
        "import nltk\n",
        "import re\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tag import pos_tag\n",
        "import spacy\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "nltk.download('stopwords')\n",
        "nlp=spacy.load('en_core_web_sm')"
      ],
      "metadata": {
        "id": "eaqXZw0aVc4c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(file_path)\n",
        "text= df['query'].dropna().tolist()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "id": "frqmMEbOYU6l",
        "outputId": "c5861244-273f-4933-970b-f6b611edcfa3"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'file_path' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-47a7b3a01ee9>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'query'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'file_path' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Lowercasing and normalize texts\n",
        "def normalize_text(text):\n",
        "\n",
        "  text=text.lower()\n",
        "  text=re.sub(r'(.)\\1+',r'\\1',text) #remove redundant letters\n",
        "  text=re.sub(r'\\bu\\b','you',text) #Replacing abbreviations\n",
        "  text=re.sub(r'\\bplz\\b','please',text) #Replacing abbreviations\n",
        "  text=re.sub(r'\\bWhr\\b','Where',text) #Replacing abbreviations\n",
        "  text=re.sub(r'\\bordr\\b','order',text) #Replacing abbreviations\n",
        "  text=re.sub(r'\\baf\\b','as f**k',text) #Replacing abbreviations\n",
        "  return text\n",
        "\n",
        "normalzd_text= normalize_text(text)\n",
        "print(\"Text normalized text:\",normalzd_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "id": "lUN9XEFrUU2t",
        "outputId": "191b277e-5f6b-4374-d471-2eb202a11177"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'text' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-7783c176bf7c>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mnormalzd_text\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mnormalize_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Text normalized text:\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnormalzd_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'text' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Stopwords removal and lemmatization\n",
        "\n",
        "def stopword_remove(text):\n",
        "\n",
        "  tokens=word_tokenize(text)\n",
        "\n",
        "  print(\"Tokens:\", tokens)\n",
        "\n",
        "  stop_words=set(stopwords.words('english'))\n",
        "\n",
        "  filtered_tokens=[word for word in tokens if word.lower() not in stop_words]\n",
        "\n",
        "  filtered_sentence=' '.join(filtered_tokens)\n",
        "\n",
        "  return filtered_sentence\n",
        "\n",
        "def lemmatized_text(text):\n",
        "\n",
        "  doc=nlp(text)\n",
        "\n",
        "  lemmatized_tokens=[token.lemma_ for token in doc]\n",
        "\n",
        "  lemmatized_text=' '.join(lemmatized_tokens)\n",
        "\n",
        "  print(\"Lemmatized text:\",lemmatized_text)\n",
        "\n",
        "  return lemmatized_text"
      ],
      "metadata": {
        "id": "IZhQ5Tp6YERu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Final processed text\n",
        "\n",
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = normalize_text(text)\n",
        "    text = stopword_remove(text)\n",
        "    text = lemmatized_text(text)\n",
        "    return text.strip()\n",
        "\n",
        "cleaned_queries = [clean_text(q) for q in text]"
      ],
      "metadata": {
        "id": "CsWBZ5RRYUHN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}